{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ik5QWtCgBpjO",
    "outputId": "475deb08-660c-4235-9d61-9deefa8c6881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V-Im8mJ-CCtR"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# loading data and opening our input data in the form of a txt file\n",
    "# project Gutenburg/berg is where the data can be found (Just Google it!)\n",
    "file = open(\"frankenstein-2.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RMPBKUgLCLZe"
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# standardization\n",
    "# what is tokenization? Tokenization is the process of breaking a stream of text up into words phrases symbols or other meaningful elements\n",
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenizing the text into token\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    # filtering the stopwords using lambda\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FsiKS7oCCVLP"
   },
   "outputs": [],
   "source": [
    "# preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hkw_M5pBCiiC"
   },
   "outputs": [],
   "source": [
    "# chars to numbers\n",
    "# convert characters in our input to numbers\n",
    "# we'll sort the list of the set of all characters that appear in out i/p text and then use the enumerate fn to get numbers that represent the characters\n",
    "# we'll then create a dictionary that stores the keys and values, or the characters and the numbers that represent them \n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RampyBVZPxIW",
    "outputId": "6765a697-ed18-43d0-ef84-071ae64f2ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "# check if words to chars or chars to num (?!) worked?\n",
    "# just so we get an idea of whether our process of converting words to characters has worked,\n",
    "# we print the length of our variables \n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AbkdhXpGP4i9"
   },
   "outputs": [],
   "source": [
    "# seq length\n",
    "# we're defining how long we want an individual sequence here\n",
    "# an individual sequence is a complete mapping of input characters as integers\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TZhOpriCP8JH"
   },
   "outputs": [],
   "source": [
    "# loop through the sequence\n",
    "# here we're going through the entire list of i/ps and converting the chars to numbers with a for loop\n",
    "# this will create a bunch of sequences where each sequence starts with the next character in the i/p data\n",
    "# beginning with the first character\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # define input and output sequences\n",
    "    # input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # converting list of characters to integers based on previous values and appending the value to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpDE5pejQDX6",
    "outputId": "38370eed-c85f-4473-f9ef-659c847ee401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269466\n"
     ]
    }
   ],
   "source": [
    "# check to see how many total input sequences we have\n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VzihOXTDQD-1"
   },
   "outputs": [],
   "source": [
    "# convert into sequence to np array that our network can use\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oxnjPlx5QG5h"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding our label data\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3wAPmuLSQKN-"
   },
   "outputs": [],
   "source": [
    "# creating the model\n",
    "# creating a sequential model\n",
    "# dropout is used to prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-B0LeZC-QNuC"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_v2Cupp1QQhf"
   },
   "outputs": [],
   "source": [
    "# saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h27rKpw_QTzK",
    "outputId": "0923feb4-54c6-4dc9-8f01-45016b3654ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1053/1053 [==============================] - 178s 159ms/step - loss: 2.8704\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.87036, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "1053/1053 [==============================] - 168s 159ms/step - loss: 2.6004\n",
      "\n",
      "Epoch 00002: loss improved from 2.87036 to 2.60035, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "1053/1053 [==============================] - 168s 159ms/step - loss: 2.4468\n",
      "\n",
      "Epoch 00003: loss improved from 2.60035 to 2.44685, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "1053/1053 [==============================] - 168s 159ms/step - loss: 2.3268\n",
      "\n",
      "Epoch 00004: loss improved from 2.44685 to 2.32681, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13756e4590>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model and let it train\n",
    "model.fit(X, y, epochs=4, batch_size=256, callbacks=desired_callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Uka2pL5LcVca"
   },
   "outputs": [],
   "source": [
    "# recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NiP5mGcacbld"
   },
   "outputs": [],
   "source": [
    "# output of the model back into characters\n",
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEBm0c0UcexA",
    "outputId": "957dd6f1-9138-4c7e-b5ba-bfa813ea9d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" ay fervour warmed gladly would sacrifice fortune existence every hope furtherance enterprise one man \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help generate\n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gA0K6bT5ch5F",
    "outputId": "1439440b-c06e-4f0d-bab0-9f36f74d33b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare sea"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3Q28Wruw4zX"
   },
   "source": [
    "Does this seem somewhat disappointing? Yes, the text that was generated doesn't make any sense, and it seems to start simply repeating patterns after a little bit. However, the longer you train the network the better the text that is generated will be.\n",
    "\n",
    "For instance, when the number of training epochs was increased to 20, the output looked more like this:\n",
    "\n",
    "\"ligther my paling the same been the this manner to the forter the shempented and the had an ardand the verasion the the dears conterration of the astore\"\n",
    "\n",
    "The model is now generating actual words, even if most of it still doesn't make sense. Still, for only around 100 lines of code, it isn't bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGeC1ObfxbPl"
   },
   "source": [
    "***Conclusion***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnTz_rnmxXWA"
   },
   "source": [
    "You'll want to increase the number of training epochs to improve the network's performance. However, you may also want to use either a deeper neural network (add more layers to the network) or a wider network (increase the number of neurons/memory units) in the layers.\n",
    "\n",
    "You could also try adjusting the batch size, one hot-encoding the inputs, padding the input sequences, or combining any number of these ideas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
